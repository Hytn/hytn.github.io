@inproceedings{chen2024rdbench,
title={{RD}2Bench: Toward Data-Centric Automatic R\&D},
author={Haotian Chen and Xinjie Shen and Zeqi Ye and Xiao Yang and Xu Yang and Weiqing Liu and Jiang Bian},
booktitle={ICLR 2024 Workshop: How Far Are We From AGI},
year={2024},
url={https://openreview.net/forum?id=QIY7LtSeOM}
}

@inproceedings{chen2024oodreb,
author = {Chen, Haotian and Guo, Houjing and Chen, Bingsheng and Zhou, Xiangdong},
title = {OODREB: Benchmarking State-of-the-Art Methods for Out-Of-Distribution Generalization on Relation Extraction},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645695},
doi = {10.1145/3589334.3645695},
abstract = {Relation extraction (RE) methods have achieved striking performance when training and test data are independently and identically distributed (i.i.d). However, in real-world scenarios where RE models are trained to acquire knowledge in the wild, the assumption can hardly be satisfied due to the different and unknown testing distributions. In this paper, we serve as the first effort to study out-of-distribution (OOD) problems in RE by constructing an out-of-distribution relation extraction benchmark (OODREB) and then investigating the abilities of state-of-the-art (SOTA) RE methods on OODREB in both i.i.d. and OOD settings. Our proposed benchmark and analysis reveal new findings and insights: (1) Existing SOTA RE methods struggle to achieve satisfying performance on OODREB in both i.i.d. and OOD settings due to the complex training data and biased model selection method. Rethinking the developing protocols of RE methods is of great urgency. (2) The SOTA RE methods fail to learn causality due to the diverse linguistic expressions of causal information. The failure limits their robustness and generalization ability; (3) Current RE methods based on language models are far away from being deployed in real-world applications. We appeal to future work to take the OOD generalization and causality learning ability into consideration. We make our annotation and code publicly available at https://github.com/Hytn/OODREB.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2294--2303},
numpages = {10},
keywords = {benchmark, out-of-distribution generalization, relation extraction},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{chen2024rethinking, 
title={Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study}, 
volume={38}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/30086}, 
DOI={10.1609/aaai.v38i19.30086}, 
number={19}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Chen, Haotian and Zhang, Lingwei and Liu, Yiran and Yu, Yang}, 
year={2024}, 
month={Mar.}, 
pages={20958--20966} 
}

@incollection{chen2023salas,
  title = {{{SALAS}}: {{Supervised Aspect Learning Improves Abstractive Multi-document Summarization Through Aspect Information Loss}}},
  shorttitle = {{{SALAS}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}: {{Research Track}}},
  author = {Chen, Haotian and Zhang, Han and Guo, Houjing and Yi, Shuchang and Chen, Bingsheng and Zhou, Xiangdong},
  editor = {Koutra, Danai and Plant, Claudia and Gomez Rodriguez, Manuel and Baralis, Elena and Bonchi, Francesco},
  year = {2023},
  volume = {14172},
  pages = {55--70},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-43421-1_4},
  urldate = {2023-10-10},
  isbn = {978-3-031-43420-4 978-3-031-43421-1},
  langid = {english},
}


@inproceedings{chen2023did,
  title = {Did the {{Models Understand Documents}}? {{Benchmarking Models}} for {{Language Understanding}} in {{Document-Level Relation Extraction}}},
  shorttitle = {Did the {{Models Understand Documents}}?},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Haotian and Chen, Bingsheng and Zhou, Xiangdong},
  year = {2023},
  pages = {6418--6435},
  publisher = {{Association for Computational Linguistics}},
  address = {{Toronto, Canada}},
  doi = {10.18653/v1/2023.acl-long.354},
  urldate = {2023-08-10},
  langid = {english},
}
